from pipeline.tracker import PipelineTracker
from data_forge.forge import DataForgeEngine
from type_engine.detector import SemanticDetector
from type_engine.cleaner import AutoCleaner

class Dataset:
    def __init__(self, name, dataframe, temporary=False):
        self.name = name
        self.df = dataframe
        self.is_temporary = temporary
        self.history = [dataframe.copy()]  # Store history for undo/reset
        
        self.detector = SemanticDetector()
        self.cleaner = AutoCleaner()
        
        self.tracker = PipelineTracker()
        self.tracker.add_step("Dataset Created / Loaded", {"rows": len(dataframe), "cols": len(dataframe.columns)})

    def auto_clean(self):
        """Detect types and use the AutoCleaner to remove mismatches"""
        types = self.detector.detect_all_column_types(self.df)
        self.df = self.cleaner.handle_invalid_values(self.df, types)
        
        # Pull the operations log generated by the cleaner
        logs = self.cleaner.get_operations_log()
        self.cleaner.clear_operations_log()
        
        # Record everything in the tracker pipeline
        if logs:
            for log in logs:
                self.save_state(operation_name=f"AutoClean: {log}")
        else:
            self.save_state(operation_name="AutoClean: No invalid data found")
        return logs

    def save_state(self, operation_name="Operation", stats=None):
        """Save current state to history and pipeline"""
        self.history.append(self.df.copy())
        if stats is None:
            stats = {"rows": len(self.df), "cols": len(self.df.columns)}
        self.tracker.add_step(operation_name, stats)

    def undo(self):
        """Undo last operation"""
        if len(self.history) > 1:
            self.df = self.history[-2].copy()
            self.history = self.history[:-1]
            self.tracker.add_step("Undo", {"rows": len(self.df)})
            return True
        return False

    def reset(self):
        """Reset to original state"""
        if len(self.history) > 1:
            self.df = self.history[0].copy()
            self.history = [self.history[0].copy()]  # Reset history to just original
            self.tracker.clear()
            self.tracker.add_step("Reset to Original", {"rows": len(self.df), "cols": len(self.df.columns)})
            return True
        return False

class DatasetManager:
    def __init__(self):
        self.datasets = {}  # name -> Dataset
        self.active_dataset_name = None
        self.forge = DataForgeEngine()

    def add_dataset(self, name, df, temporary=False):
        """Add a new dataset"""
        self.datasets[name] = Dataset(name, df, temporary)
        if self.active_dataset_name is None:
            self.active_dataset_name = name

    def get_active_dataset(self):
        """Return the currently active dataset"""
        if self.active_dataset_name:
            return self.datasets[self.active_dataset_name]
        return None

    def apply_basic_op(self, op_func):
        """Apply a per-file operation to the active dataset"""
        ds = self.get_active_dataset()
        if ds:
            ds.df = op_func(ds.df)

    def apply_cross_file_op(self, selected_names, op_func, result_name):
        """Apply a cross-file operation on selected datasets"""
        dfs = [self.datasets[name].df for name in selected_names if name in self.datasets]
        if not dfs:
            return None
        result_df = op_func(dfs)
        self.add_dataset(result_name, result_df, temporary=True)
        self.active_dataset_name = result_name
        return result_df
